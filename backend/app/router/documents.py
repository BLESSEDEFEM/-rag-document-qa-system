import json
import logging
from datetime import datetime
from typing import List, Dict, Any

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from sqlalchemy.orm import Session

# Import database dependencies
from app.database import get_db
from app.models import Document

# Import services
from app.services import file_storage, text_extraction
from app.services.chunking_service import chunk_text
from app.services.embedding_service import embedding_service
from app.services.pinecone_service import pinecone_service
from app.services.llm_service import llm_service
from app.services.cache_service import cache_service
from pydantic import BaseModel
from fastapi.responses import StreamingResponse
from app.middleware.auth import get_current_user, get_current_user_optional

# Request models
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    min_score: float = 0.3
    document_id: int | None = None

logger = logging.getLogger(__name__)

router = APIRouter(
    tags=["documents"],
    responses={404: {"description": "Not found"}}
)


@router.post("/upload", response_model=Dict[str, Any])
async def upload_document(
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """Upload a document (PDF, DOCX, TXT) for processing."""

    ALLOWED_EXTENSIONS = {".pdf", ".docx", ".txt"}
    MAX_FILE_SIZE = 10 * 1024 * 1024

    logger.info("Upload attempt: %s", file.filename)

    file_extension = None
    if file.filename:
        file_extension = "." + file.filename.split(".")[-1].lower()

    if file_extension not in ALLOWED_EXTENSIONS:
        logger.warning("Invalid file type attempted: %s", file.filename)
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type. Allowed: {', '.join(ALLOWED_EXTENSIONS)}")

    contents = await file.read()
    file_size = len(contents)

    if file_size > MAX_FILE_SIZE:
        logger.warning("File too large: %d bytes", file_size)
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024)} MB")

    await file.seek(0)

    try:
        logger.info("Saving file to disk...")
        file_path, unique_filename = await file_storage.save_uploaded_file(file)
        logger.info("File saved: %s", file_path)
    except Exception as e:
        logger.error("Error saving file: %s", e)
        raise HTTPException(
            status_code=500,
            detail=f"Error saving file: {str(e)}"
        ) from e

    logger.info("Extracting text from document...")
    extraction_result = await text_extraction.extract_text(
        file_path,
        file_extension
    )

    extracted_text = None
    page_count = None
    chunks = None
    embeddings = None
    chunk_count = None
    status = "failed"

    if extraction_result["success"]:
        logger.info(
            "Text extraction successful: %d characters", len(
                extraction_result['text']))
        extracted_text = extraction_result["text"]
        page_count = extraction_result["page_count"]
        status = "extracted"

        try:
            logger.info("Chunking text...")
            chunks = chunk_text(extracted_text, chunk_size=1000, overlap=100)
            chunks = [chunk.replace('\x00', '') for chunk in chunks] if chunks else None
            chunk_count = len(chunks)
            logger.info("Created %d chunks", chunk_count)

            MAX_CHUNKS = 200
            truncated = False
            original_chunk_count = chunk_count

            if chunk_count > MAX_CHUNKS:
                logger.warning(f"Document has {chunk_count} chunks, truncating to {MAX_CHUNKS}")
                chunks = chunks[:MAX_CHUNKS]
                chunk_count = MAX_CHUNKS
                truncated = True

            logger.info("Embeddings will be auto-generated by Pinecone")
            embeddings = None

            status = "ready"

        except Exception as e:
            logger.error("Error during chunking/embedding: %s", e)
            status = "extracted"

    else:
        logger.warning(
            "Text extraction failed: %s",
            extraction_result['error'])

    try:
        logger.info("Creating database record...")

        document = Document(
            user_id=user["sub"],
            filename=unique_filename,
            original_filename=file.filename or "unknown",
            file_path=file_path,
            file_size=file_size,
            file_type=file_extension.replace(".", ""),
            extracted_text=extracted_text.replace('\x00', '') if extracted_text else None,
            page_count=page_count,
            status=status,
            chunks=json.dumps(chunks) if chunks else None,
            embeddings=None,
            chunk_count=chunk_count,
            embedding_model="cohere-embed-v3" if chunks else None,
            embedding_dimension=1024 if chunks else None,
            embedding_date=datetime.utcnow() if chunks else None
        )

        db.add(document)
        db.commit()
        db.refresh(document)

        logger.info("Database record created: Document ID %d", document.id)

        if chunks and document.id:
            try:
                logger.info("Generating embeddings for %d chunks...", len(chunks))
                embeddings = embedding_service.generate_embeddings(chunks)
                
                if embeddings and all(embeddings):
                    logger.info("Storing %d chunks in Pinecone...", len(chunks))
                    
                    result = pinecone_service.upsert_embeddings(
                        document_id=document.id,
                        chunks=chunks,
                        embeddings=embeddings,
                        metadata={
                            "filename": document.original_filename,
                            "file_type": document.file_type
                        }
                    )
                    
                    if result["success"]:
                        logger.info("Pinecone storage successful: %d vectors", result["upserted_count"])
                    else:
                        logger.error("Pinecone storage failed: %s", result.get("error"))
                else:
                    logger.error("Failed to generate embeddings")
                    
            except Exception as e:
                logger.error("Error storing in Pinecone: %s", e)

    except Exception as e:
        logger.error("Error creating database record: %s", e)
        db.rollback()
        await file_storage.delete_file(file_path)
        raise HTTPException(
            status_code=500,
            detail=f"Error saving to database: {str(e)}"
        ) from e
        
    cache_service.delete("documents:list")

    return {
        "message": "Document uploaded and processed successfully" if not truncated else 
                   f"⚠️ Document partially processed: First {MAX_CHUNKS} chunks only (file had {original_chunk_count} chunks)",
        "document_id": document.id,
        "filename": document.original_filename,
        "unique_filename": document.filename,
        "size_bytes": document.file_size,
        "file_type": document.file_type,
        "status": document.status,
        "page_count": document.page_count,
        "character_count": len(extracted_text) if extracted_text else 0,
        "chunk_count": document.chunk_count,
        "truncated": truncated if 'truncated' in locals() else False,
        "original_chunk_count": original_chunk_count if 'original_chunk_count' in locals() else chunk_count,
        "chunks_processed": chunk_count,
        "embedding_dimension": document.embedding_dimension,
        "is_embedded": document.chunk_count is not None and document.chunk_count > 0,
        "upload_date": document.upload_date.isoformat(),
        "extracted_text_preview": extracted_text[:200] + "..." if extracted_text and len(extracted_text) > 200 else extracted_text
    }


@router.post("/answer", response_model=Dict[str, Any])
async def answer_question(
    request: QueryRequest,
    db: Session = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    query = request.query
    top_k = request.top_k
    min_score = request.min_score
    document_id = request.document_id

    logger.info(f"Answer request: '{query}' (document_id={document_id})")

    if not query or len(query.strip()) == 0:
        raise HTTPException(status_code=400, detail="Query cannot be empty")

    if len(query) > 1000:
        raise HTTPException(status_code=400,
                            detail="Query too long (max 1000 characters)")

    try:
        logger.info("Generating query embedding...")
        query_embedding = embedding_service.generate_embedding(query.strip())
        
        if not query_embedding:
            raise HTTPException(500, "Failed to generate query embedding")

        logger.info("Searching Pinecone for top %d matches...", top_k)
        
        # Build filter
        if document_id:
            doc = db.query(Document).filter(
                Document.id == document_id,
                Document.user_id == user["sub"]
            ).first()
            
            if not doc:
                return {
                    "success": True,
                    "answer": "Document not found or you don't have access to it.",
                    "query": query,
                    "chunks_used": 0,
                    "sources": []
                }
            
            pinecone_filter = {"document_id": document_id}
        else:
            user_docs = db.query(Document.id).filter(
                Document.user_id == user["sub"],
                Document.is_deleted == False
            ).all()
            user_doc_ids = [doc.id for doc in user_docs]
            
            if not user_doc_ids:
                return {
                    "success": True,
                    "answer": "You haven't uploaded any documents yet. Please upload documents first.",
                    "query": query,
                    "chunks_used": 0,
                    "sources": []
                }
            
            pinecone_filter = {"document_id": {"$in": user_doc_ids}}
        
        pinecone_results = pinecone_service.query_similar(
            query_embedding=query_embedding,
            top_k=top_k,
            filter_dict=pinecone_filter
        )

        context_chunks = []
        for match in pinecone_results:
            chunk = {
                "chunk_text": match["metadata"].get("chunk_text", ""),
                "score": match["score"],
                "source": {
                    "document": match["metadata"].get("document_id"),
                    "filename": match["metadata"].get("filename", "Unknown"),
                    "file_type": match["metadata"].get("file_type", "Unknown"),
                    "chunk_index": match["metadata"].get("chunk_index", 0)
                }
            }
            context_chunks.append(chunk)

        context_chunks = [c for c in context_chunks if c["score"] >= min_score]
        logger.info("After filtering: %d chunks", len(context_chunks))

        if len(context_chunks) == 0:
            return {
                "success": True,
                "answer": "I couldn't find any relevant information in your documents to answer your question. Please try rephrasing or upload related documents.",
                "query": query,
                "chunks_used": 0,
                "sources": []}

        logger.info("Generating answer with %d chunks...", len(context_chunks))
        llm_result = llm_service.generate_answer(
            query=query,
            context_chunks=context_chunks,
            max_chunks=5
        )

        if not llm_result["success"]:
            raise HTTPException(
                status_code=500,
                detail=f"Error generating answer: {llm_result.get('error')}"
            )

        logger.info("Answer generated successfully")

        return {
            "success": True,
            "answer": llm_result["answer"],
            "query": query,
            "chunks_used": llm_result["chunks_used"],
            "sources": llm_result["sources"],
            "retrieval_stats": {
                "chunks_retrieved": len(pinecone_results),
                "chunks_after_filter": len(context_chunks),
                "top_k": top_k,
                "min_score": min_score
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error("Error processing answer request: %s", e)
        raise HTTPException(
            status_code=500,
            detail=f"Error processing answer request: {str(e)}"
        ) from e


@router.get("/list", response_model=List[Dict[str, Any]])
async def list_documents(
    db: Session = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """List all uploaded documents."""
    logger.info(f"Document list requested by user: {user['sub']}")
    
    cache_key = f"documents:list:{user['sub']}"
    cached = cache_service.get(cache_key)
    if cached:
        logger.info("Returning cached document list")
        return cached
    
    documents = db.query(Document).filter(
        Document.is_deleted == False,
        Document.user_id == user["sub"]
    ).all()
    
    result = [
        {
            "id": doc.id,
            "filename": doc.original_filename,
            "unique_filename": doc.filename,
            "file_type": doc.file_type,
            "file_size": doc.file_size,
            "status": doc.status,
            "page_count": doc.page_count,
            "upload_date": doc.upload_date.isoformat(),
            "character_count": len(doc.extracted_text) if doc.extracted_text else 0
        }
        for doc in documents
    ]
    
    cache_service.set(cache_key, result, ttl=300)
    
    return result


@router.delete("/{document_id}")
async def delete_document(
    document_id: int,
    db: Session = Depends(get_db),
    user: dict = Depends(get_current_user)
):
    """Delete a document."""
    document = db.query(Document).filter(
        Document.id == document_id,
        Document.user_id == user["sub"]
    ).first()
    
    if not document:
        raise HTTPException(404, "Document not found")

    db.delete(document)
    db.commit()

    pinecone_service.delete_document_vectors(document_id)
    await file_storage.delete_file(document.file_path)
    
    cache_service.delete(f"documents:list:{user['sub']}")

    return {"message": "Document deleted", "document_id": document_id}


@router.get("/health")
async def documents_health():
    """Health check for documents router."""
    return {
        "status": "healthy",
        "router": "documents",
        "timestamps": datetime.utcnow().isoformat()
    }
